<!DOCTYPE html><html><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="format-detection" content="telephone=no"/><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><title class="next-head">Mehrad Faridan | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" class="next-head"/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:title" content="Mehrad Faridan | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:site_name" content="University of Calgary Interactions Lab" class="next-head"/><meta property="og:url" content="https://ilab.ucalgary.ca/" class="next-head"/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/people/mehrad-faridan.jpg" class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:title" content="Mehrad Faridan | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/people/mehrad-faridan.jpg" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@ucalgary" class="next-head"/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" class="next-head"/><link rel="preload" href="/_next/static/nTZbgTZuc92zJuhx8A5mV/pages/person.js" as="script"/><link rel="preload" href="/_next/static/nTZbgTZuc92zJuhx8A5mV/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.2ccf7861fac39e850a30.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-ceb4d64d798d2348aa74.js" as="script"/></head><body><div id="__next"><div><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="person" class="category" style="text-align:center"><img class="ui circular image large-profile" src="/static/images/people/mehrad-faridan.jpg" style="margin:auto"/><h1>Mehrad Faridan</h1><p></p><p><a href="https://www.mehradfaridan.com/" target="_blank"><i class="fas fa-link fa-fw"></i>https://www.mehradfaridan.com/</a></p><p><a href="https://scholar.google.com/citations?user=amh7v2EAAAAJ" target="_blank"><i class="fas fa-graduation-cap fa-fw"></i>Google Scholar</a></p><div class="ui horizontal small divided link list"><div class="item"><a href="https://twitter.com/MehradFaridan" target="_blank" style="font-size:1.2em"><i class="fab fa-twitter fa-fw"></i>MehradFaridan</a></div><div class="item"><a href="https://github.com/mehradFaridan" target="_blank" style="font-size:1.2em"><i class="fab fa-github-alt fa-fw"></i>mehradFaridan</a></div><div class="item"><a href="https://www.linkedin.com/in/mehrad-f-a34462164/" target="_blank" style="font-size:1.2em"><i class="fab fa-linkedin-in fa-fw"></i>LinkedIn</a></div><div class="item"><a href="mehrad.faridan1@ucalgary.ca" target="_blank" style="font-size:1.2em"><i class="far fa-envelope fa-fw"></i>mehrad.faridan1@ucalgary.ca</a></div></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="uist-2023-ihara"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b></p><p><a href="/people/keiichi-ihara"><img src="/static/images/people/keiichi-ihara.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Keiichi Ihara</span></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <span>Ayumi Ichikawa</span> , <span>Ikkaku Kawaguchi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-faridan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b></p><p><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <a href="/people/bheesha-kumari"><img src="/static/images/people/bheesha-kumari.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Bheesha Kumari</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-kaimoto"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b></p><p><a href="/people/hiroki-kaimoto"><img src="/static/images/people/hiroki-kaimoto.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Hiroki Kaimoto</span></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <span>Jiatong Li</span> , <a href="/people/samin-farajian"><img src="/static/images/people/samin-farajian.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Samin Farajian</span></a> , <span>Yasuaki Kakehi</span> , <span>Ken Nakagaki</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-sic-2022-faridan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-sic-2022-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST SIC 2022</span><span class="ui big basic pink label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</b></p><p><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Marcus Friedel</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Ultrasound Transducers</span><span class="ui brown basic label">Robotics</span></div></p></div></div></div><div id="publications-modal"><div id="uist-2023-ihara" class="ui large modal"><div class="header"><a href="/publications/uist-2023-ihara" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-ihara</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-ihara" target="_blank">HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</a></h1><p class="meta"><a href="/people/keiichi-ihara"><img src="/static/images/people/keiichi-ihara.jpg" class="ui circular spaced image mini-profile"/><strong>Keiichi Ihara</strong></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <span>Ayumi Ichikawa</span> , <span>Ikkaku Kawaguchi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-2023-ihara.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-ihara.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/KSBPtiXy8Hg" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/KSBPtiXy8Hg?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/KSBPtiXy8Hg/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces HoloBots, a mixed reality remote collaboration system that augments holographic telepresence with synchronized mobile robots. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also physically engage with local users and their environment. HoloBots allows the users to touch, grasp, manipulate, and interact with the remote physical environment as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond the existing physical telepresence, HoloBots contributes to an exploration of broader design space, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system with twelve participants by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results confirm that our system significantly enhances the level of co-presence and shared experience, compared to the other conditions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Keiichi Ihara<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Ayumi Ichikawa<!-- -->, <!-- -->Ikkaku Kawaguchi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3586183.3606727" target="_blank">https://doi.org/10.1145/3586183.3606727</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-faridan" class="ui large modal"><div class="header"><a href="/publications/chi-2023-faridan" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2023-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-faridan" target="_blank">ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <a href="/people/bheesha-kumari"><img src="/static/images/people/bheesha-kumari.jpg" class="ui circular spaced image mini-profile"/><strong>Bheesha Kumari</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/chi-2023-faridan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2023-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/VOe3fETd3sk" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/VOe3fETd3sk?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/VOe3fETd3sk/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to the existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches (e.g. ChameleonMask), we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality (MR) hand gestural navigation and verbal communication. By overlaying the remote instructor&#x27;s virtual hands in the local user&#x27;s MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively ``teleoperating&#x27;&#x27; a real human. We evaluate our system through the in-the-wild deployment for physiotherapy classrooms, as well as lab-based experiments for other application domains such as mechanical assembly, sign language, and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Bheesha Kumari<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544548.3581381" target="_blank">https://doi.org/10.1145/3544548.3581381</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-kaimoto" class="ui large modal"><div class="header"><a href="/publications/uist-2022-kaimoto" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-kaimoto</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-kaimoto" target="_blank">Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</a></h1><p class="meta"><a href="/people/hiroki-kaimoto"><img src="/static/images/people/hiroki-kaimoto.jpg" class="ui circular spaced image mini-profile"/><strong>Hiroki Kaimoto</strong></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <span>Jiatong Li</span> , <a href="/people/samin-farajian"><img src="/static/images/people/samin-farajian.jpg" class="ui circular spaced image mini-profile"/><strong>Samin Farajian</strong></a> , <span>Yasuaki Kakehi</span> , <span>Ken Nakagaki</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-2022-kaimoto.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-kaimoto.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/xy-IeVgoEpY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/xy-IeVgoEpY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/xy-IeVgoEpY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to affect each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hiroki Kaimoto<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Jiatong Li<!-- -->, <!-- -->Samin Farajian<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Ken Nakagaki<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-sic-2022-faridan" class="ui large modal"><div class="header"><a href="/publications/uist-sic-2022-faridan" target="_blank"><i class="fas fa-link fa-fw"></i>uist-sic-2022-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST SIC 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-sic-2022-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-sic-2022-faridan" target="_blank">UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><strong>Marcus Friedel</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-sic-2022-faridan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-sic-2022-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jMYAQzzQ_PI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jMYAQzzQ_PI?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/jMYAQzzQ_PI/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce UltraBots, a system that combines ultrasound haptic feedback and robotic actuation for large-area mid-air haptics for VR. Ultrasound haptics can provide precise mid-air haptic feedback and versatile shape rendering, but the interaction area is often limited by the small size of the ultrasound devices, restricting the possible interactions for VR. To address this problem, this paper introduces a novel approach that combines robotic actuation with ultrasound haptics. More specifically, we will attach ultrasound transducer arrays to tabletop mobile robots or robotic arms for scalable, extendable, and translatable interaction areas. We plan to use Sony Toio robots for 2D translation and/or commercially available robotic arms for 3D translation. Using robotic actuation and hand tracking measured by a VR HMD (ex: Oculus Quest), our system can keep the ultrasound transducers underneath the user’s hands to provide on-demand haptics. We demonstrate applications with workspace environments, medical training, education and entertainment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Ultrasound Transducers</span><span class="ui brown basic label">Robotics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Marcus Friedel<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</b>. <i>In Adjunct undefined (UIST SIC &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->3<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526114.3561350" target="_blank">https://doi.org/10.1145/3526114.3561350</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"mehrad-faridan"}},"page":"/person","query":{"id":"mehrad-faridan"},"buildId":"nTZbgTZuc92zJuhx8A5mV","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/person" src="/_next/static/nTZbgTZuc92zJuhx8A5mV/pages/person.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/nTZbgTZuc92zJuhx8A5mV/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.2ccf7861fac39e850a30.js" async=""></script><script src="/_next/static/runtime/main-ceb4d64d798d2348aa74.js" async=""></script></body></html>